## Реальное применение CROSS JOIN в бою

#### Исходные данные:
- портфель клиентов (id, параметры клиентов), размерность > 200 млн. записей
- таблицы с агрегатами клиентов на разные даты

#### Задача:
- получить итоговый портфель с параметрами клиентов + все агрегаты из других таблиц на те даты, за которые эти агрегаты существуют для клиента.

#### Решение:

Если пробовать в лоб к портфелю приклеить данные из других таблиц по равенству дат, то в бОльшей вероятностью такие JOIN дадут пустую таблицу.
(никто не гарантирует, что агрегаты для клиентов есть на 1) все даты 2) на одинаковые даты).

Для решения задачи я:
1. Создаю DataFrame дат (например, это будет 2024 год).

```sql
df_dt = (
  spark.sql("""
  select explode(sequence(to_date('2024-01-01), to_date('2024-12-31'), interval 1 day)) as dt
    """)
    )
```

2. Создаю DataFrame клиентов из портфеля.

```sql
df_client = table.select("client_id")
```

3. Соединяю 2 DataFrame с помощью CROSS JOIN и пишу в parquet.

```sql
df_cross = df_client.crossJoin(df_dt)
```

4. Присоединяю и исходному портфелю df_cross, использовав Broadcast

```sql
(
  spark.sql("""
  select 
  /* broadcast(df_cross) */
    t1.*,
    t2.dt
  from table t1
  inner join df_cross t2
    on t1.client_id = t2.client_id
  
    """)
    )
```

#### В чем смысл?

Используя CROSS JOIN, мы создаём полный картезианский продукт клиентов и дат, что позволяет присоединить данные агрегатов к каждому клиенту на все возможные даты, за которые есть агрегаты. Таким образом, даже если данные агрегатов отсутствуют на какие-то даты, мы гарантируем наличие всех необходимых сочетаний "клиент-дата". Broadcast join в этом случае используется для ускорения операции за счёт передачи маленького DataFrame на все узлы кластера.


## Советы и рекомендации для оптимизации работы с большими данными в Spark

#### 1. Оптимизация объёма данных для JOIN-ов
- **Используйте фильтрацию на ранних этапах**: Перед выполнением JOIN постарайтесь отфильтровать ненужные данные. Это уменьшит объём данных для обработки и ускорит выполнение запроса.
- **Оптимизируйте выбор колонок**: Выбирайте только те столбцы, которые необходимы для дальнейших шагов. Не загружайте весь набор данных без необходимости.

#### 2. Broadcast Join
- **Используйте Broadcast Join для маленьких таблиц**: Если одна из таблиц относительно небольшая (меньше порога `spark.sql.autoBroadcastJoinThreshold`), применяйте Broadcast Join, чтобы избежать shuffle данных. Это особенно полезно при работе с большими клиентскими портфелями и небольшими справочными таблицами.
- **Настройка порога Broadcast**: Проверьте параметр `spark.sql.autoBroadcastJoinThreshold`. По умолчанию он равен 10 MB, но его можно увеличить для более крупных таблиц, чтобы расширить применение Broadcast Join.

#### 3. Работа с большими данными
- **Используйте разделение данных (partitioning)**: При сохранении данных в формате Parquet или других форматах используйте разделение по часто используемым полям, таким как дата. Это ускоряет выполнение запросов, которые работают с отдельными временными периодами.
- **Ограничьте размер файлов**: Большие файлы могут замедлить процессы чтения и записи. Разделяйте данные на более мелкие файлы для облегчения обработки в Spark.
