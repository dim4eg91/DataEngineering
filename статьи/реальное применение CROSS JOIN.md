## Реальное применение CROSS JOIN

#### Исходные данные:
- Spark
- портфель "table" (поля: id, параметры), размерность > 100 млн. записей
- другие таблицы с агрегатами этих id на разные даты

#### Задача:
- получить итоговый портфель с параметрами id + все агрегаты из других таблиц на те даты, за которые эти агрегаты существуют для id.

#### Решение:

Если пробовать в лоб к портфелю приклеить данные из других таблиц по равенству дат, то в бОльшей вероятностью такие JOIN дадут пустую таблицу.
(никто не гарантирует, что агрегаты для id есть на 1) все даты 2) на одинаковые даты).

Для решения задачи я:
1. Создаю DataFrame дат (например, это будет 2024 год).

```sql
df_dt = (
  spark.sql("""
  select explode(sequence(to_date('2024-01-01), to_date('2024-12-31'), interval 1 day)) as dt
    """)
    )
```

2. Создаю DataFrame id из портфеля.

```sql
df_id = table.select("id")
```

3. Соединяю 2 DataFrame с помощью CROSS JOIN и пишу в parquet.

```sql
df_cross = df_id.crossJoin(df_dt)
```

4. Присоединяю и исходному портфелю df_cross

```sql
(
  spark.sql("""
  select 
    t1.*,
    t2.dt
  from table t1
  inner join df_cross t2
    on t1.id = t2.id
  
    """)
    )
```

#### В чем смысл?

Используя CROSS JOIN, мы создаём полный продукт всех id и дат, что позволяет присоединить данные агрегатов к каждому id исходного портфеля на все возможные даты, за которые есть агрегаты. Таким образом, даже если данные агрегатов отсутствуют на какие-то даты, мы гарантируем наличие всех необходимых сочетаний "id-дата".


## Советы и рекомендации для оптимизации работы с большими данными в Spark

#### 1. Оптимизация объёма данных для JOIN-ов
- **Используйте фильтрацию на ранних этапах**: Перед выполнением JOIN постарайтесь отфильтровать ненужные данные. Это уменьшит объём данных для обработки и ускорит выполнение запроса.
- **Оптимизируйте выбор колонок**: Выбирайте только те столбцы, которые необходимы для дальнейших шагов. Не загружайте весь набор данных без необходимости.

#### 2. Broadcast Join
- **Используйте Broadcast Join для маленьких таблиц**: Если одна из таблиц относительно небольшая (меньше порога `spark.sql.autoBroadcastJoinThreshold`), применяйте Broadcast Join, чтобы избежать shuffle данных. Это особенно полезно при работе с большими клиентскими портфелями и небольшими справочными таблицами.
- **Настройка порога Broadcast**: Проверьте параметр `spark.sql.autoBroadcastJoinThreshold`. По умолчанию он равен 10 MB, но его можно увеличить для более крупных таблиц, чтобы расширить применение Broadcast Join.

#### 3. Работа с большими данными
- **Используйте разделение данных (partitioning)**: При сохранении данных в формате Parquet или других форматах используйте разделение по часто используемым полям, таким как дата. Это ускоряет выполнение запросов, которые работают с отдельными временными периодами.
- **Ограничьте размер файлов**: Большие файлы могут замедлить процессы чтения и записи. Разделяйте данные на более мелкие файлы для облегчения обработки в Spark.
